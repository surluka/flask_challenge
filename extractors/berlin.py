from playwright.sync_api import sync_playwright
import time
from bs4 import BeautifulSoup

# AI 는 절때 사용하지 않는다! 무조건 찾아서 한다
# 완성 될때까지 코드 리뷰를 참고하지 않는다.... 
# berlin 의 사이트를 크롤링 하는 파일.. 2개의 사이트를 걍 한꺼번에 너어도 되고 클래스로 만들어도 되지만 가급적이면 강의 내용과 유사하게 하고 추후 수정 해 보기로 한다
# 처리해야 할 내용이 꽤 되는데 skill_areas 에 있는 태그는 정적이기 때문에 requests 로 처리가 가능하지만 flask 의 검색 플레이스홀더를 사용하는 keyword 입력 방식은 동적으로 동작하기 때문에 playwright 를 사용한다 
# 3가지의 경우를 생각해야 한다..
# 1. 키워드로 검색 했을때 없을때
# 2. 키워드로 검색 했을때 한페이지에 출력 할 수 있는 페이지 수가 7개 이내 이거나(1페이지에 출력되는 페이지네이션 은 7까지가 한계 이고 넥스트와 라스트가 추가 되어 있다)
# 3. 키워드로 검색 했을때 한페이지에 출력 할 수 있는 페이지 수가 7개를 넘어가거나....(2가지 방법으로 해결이 가능하다 한페이지씩 계속 추가하여 진행이 되다가 마지막이 확인되면 중지 나머지는 첨부터 마지막을 확인하여 그만큼 반복 지금은 안한다)
# 우째 할까 생각하다가 7페이지 넘어가는건 생략 하고 다음 기회에 해보고자 한다.... playwright 는 느리기까지 하니깐...
# 안할래다가 해버렸다.... 마침 화면 상단부에 검색 아이템의 개수가 나와서 수동으로 한방에 페이지 계산이 가능해 버렸다.....
# playwrite 와 같은 종류의 모듈도 있다 셀레니움? 이라고 하는거 같은데... 이래저래 사용하기도 쉽고 신기함 단점은 지연시간인데 추출시 로딩이 다 되야 다음 작업이 되는 것도 있으리라 생각 된다
# 클라스로 활용할 내용이 없어서 애초 생각과는 다르게 걍 파일로 구분한다... 클래스로 만들어 봐야 현재 상황에선 이득될것이 없다... 만약 날짜별로 검색 내용을 정리한다거나 하면 모를까.... 현재론 메서드 컨테이너 역할 뿐이 못하는데 그건 하나마나...

#---------------------- 중요 --------------------------#
# 이 방식은 졸업챌린지와는 결과물이 다르게나오고 web3에서는 지원도 되지 않는 방식이다(web3 는 검색이 안된다).
# 미묘해 보이지만 다른 방식으로 결과물을 만들어 보고 싶어서 구지 동적 사이트를 크롤링 하는 함수를 만들었다. 이편이 더 유용하기 때문에 신경써서 만들어 둔다
# 물론 berlin 사이트도 web3 와 같은 카테고리(태그)방식의 정적사이트가 있기때문에 같은 방식으로 해도 상관은 없으나 결과물은 상이하다.(어찌보면 카테고리 크롤링?)
# 최대한 간단하고 직관적이며 씸플하게 검색어에 대한 리스트를 뽑아낸다는 목표로 작성한다

def extract_berlin_jobs(keyword):                                             # 키워드를 받는 통짜 함수로 한다  playwright의 활용은 이편이 편해 보인다
  p = sync_playwright().start()                                               # playwrite 를 시작 한다
  browser = p.chromium.launch(headless=False)                                 # 크롬(브라우저)을 킨다 동작이 되는것을 구경한다 (신기방기함...)
  page = browser.new_page()                                                   # 브라우저의 뉴페이지라고 하는 메서드로 크롬의 새로운 페이지를 하나 만든다 
  page.goto(f"https://berlinstartupjobs.com/?s={keyword}")                    # 주소창에 주소를 친다. 검색 창에 너을 필욘 없다 키워드를 바로 입력 할 수 있다 이런식으로 되어 있지 않은 사이트는 메인 사이트에서 클릭을 해 줘야 한다
  time.sleep(3.0)                                                             # 로딩이 되길 기달려야 한다... 이게 골때리는데 로딩이 끝나는 신호를 받을수 있을꺼 같은데..... 여튼 강의에는 이렇게 되어 있다 이 문제 때문에 지연시간이 걸리게 된다

  contents = page.content()                                                               # 현재 페이지를 str 형태로 받아온다

  # print(contents)                                                                       # 잘 나오는지 확인

  soup = BeautifulSoup(contents,"html.parser")                                            # 파싱을 한다

  jobs_list = []                                                                          # 최종 결과를 담을 리스트
 
  # 총 페이지 수를 알아 내어야 한다
  # 총 페이지 수는 위에 상단부에 총 아이템 수가 나온다 (이 방법이 있는줄 몰랐는데 마침 보였다.)
  item_num = int(soup.find("span", class_="ais-Stats-text").text.split()[0])                # 상단부에 나오는 총 페이지 수가 스트링 타입이니 이걸 단어 단위로 나눠서 보면 맨 앞 리스트가 전체 아이템 수 인것을 확인할 수 있다
  page_num = int(item_num/10)                                                               # 한페이지에 나오는 아이템 수가 10개이니 10으로 나눈다
  if item_num > page_num*10:                                                                # 아직 파이썬에서 나머지와 반올림 등등은 모르니 수동으로 계산해서  총 페이지를 계산해 준다
    page_num += 1                                                                           # 나머지가 있다면 형변환시 나머지가 잘린채로 변환 되었을테니 아이템 수가 더 만다면 페이지 수에 1을 더한다 
  # 페이지 수가 나옴.. 기초 함수를 모르니 시인성이 떨어짐
  # print(page_num)                                                                         # 정확하게 페이지 숫자가 나오는지 20 21 등을 너어서 출력 테스트를 해 볼 수 있다

  for i in range(page_num):                                                                 # for 또는 while break 아무거나로 해도 된다 2중 for 문으로 동작한다 그전 예제는 함수로 뺀것으로 기억...
    jobs = soup.find("ol", class_="ais-Hits-list").find_all("li", class_="ais-Hits-item")   # 처음 페이지는 무조건 스크래핑을 한다 원칙적으로는 사이트를 정하고 스크랩 해야 겠지만 스크랩할 1페이지는 이미 로딩이 되어 있다.. 그래서 순서를 바꿔서 처리해 준다
    
    # print(i+1, len(jobs))                                                                 # 페이지별로 잘 불러와 지는지 중간에 확인해 볼 수 있다.

    for job in jobs:
      title = job.find("h4", class_="bjs-jlid__h").text                                     # 타이틀을 만든다
      company = job.find("a", class_="bjs-jlid__b").text                                    # 회사이름을 만든다
      link = job.find("h4", class_="bjs-jlid__h").find("a").get("href")                     # 링크를 찾는다 태그때와 구조가 틀리다....불러올 수 있는 방법은 여러가지가 있다
      #---- 이렇게 3개만 찾는다 엑기스
      
      job = {"title":title,                                                                 # 딕셔너리를 만들어서
             "company":company,
             "link":link}
      
      jobs_list.append(job)                                                                 # 리스트에 추가해 준다

    if(i+1 < page_num):                                                                     # 마지막엔 예외처리를 하기 위함이다 마지막 페이지에선 동작 하지 않는다. 괞히 복잡해지는거 아닌가 싶지만 지연시간을 낭비하지 않기 위함이다
      page.goto(f"https://berlinstartupjobs.com/?s={keyword}&page={i+2}")                      # 다음에 스크랩 할 페이지로 이동한다 현재페이지에 1을 더하는것이니 레인지 값에 2를 더해야 한다.
      time.sleep(3)                                                                         # 이 한줄 때문에 playwright 를 사용한다 머리좋은 사람들이 requests 에 이것을 추가하지 못한단 말인가?
      contents = page.content()                                                             # 다시 콘텐츠를 불러온다 requests 의 원리와 같아 보이지만 구동방식은 완전 상이하다
      soup = BeautifulSoup(contents,"html.parser")                                          # 페이지가 바뀌었으니 다시 파싱한다
    # ---  이 if 문은 마지막 페이지에선 동작하지 않는다
    
  p.stop()                            # playwright 를 중지한다

  return jobs_list                    # 리스트 값을 반환한다


